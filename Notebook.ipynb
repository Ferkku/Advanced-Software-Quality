{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-ollama\n",
    "!pip install faiss-cpu\n",
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone repositories\n",
    "\n",
    "* Fetches a list of forks for the given base repository\n",
    "* Checks if the repositories are accesible\n",
    "* Clones the repositories to the local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize functions and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import subprocess\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "GITHUB_API_URL=\"https://api.github.com\"\n",
    "# Output directory for cloned repositories\n",
    "OUTPUT_DIR = \"repos\"\n",
    "\n",
    "load_dotenv()\n",
    "BASE_REPO = os.getenv(\"BASE_REPO\")\n",
    "API_TOKEN = os.getenv(\"GITHUB_API_TOKEN\")\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "\n",
    "def clone_repo(repo: str, output_dir: str=OUTPUT_DIR) -> None:\n",
    "    \"\"\"\n",
    "    Clones a single repository from github to OUTPUT_DIR.\n",
    "    Skips repositories that already exist in the OUTPUT_DIR.\n",
    "    \n",
    "    Args:\n",
    "        repo (str): Name of the repository given as \"user/repository_name\"\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    dir_name = repo.replace(\"/\", \"_\")\n",
    "    clone_path = f\"{output_dir}/{dir_name}\"\n",
    "    # If folder does not exist, clone the repository\n",
    "    if not os.path.exists(clone_path):\n",
    "        try:\n",
    "            print(f\"Cloning: https://github.com/{repo}\")\n",
    "            subprocess.run([\"git\", \"clone\", f\"https://github.com/{repo}\", dir_name], cwd=output_dir, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to clone: {repo}\\n{e}\")\n",
    "    else :\n",
    "        print(f\"Already cloned: {clone_path}\")\n",
    "\n",
    "def clone_repos(repos: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Loops over a list of repository names and clones them to OUTPUT_DIR\n",
    "    \n",
    "    Args:\n",
    "        repos (list[str]): List of repository names given as \"user/repository_name\"\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    for repo in repos:\n",
    "        clone_repo(repo)\n",
    "    print(\"Finished!\")\n",
    "\n",
    "def repo_exists(repo: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a repository exists on Github\n",
    "    \n",
    "    Args:\n",
    "        repo (str): Name of the repository given as \"user/repository_name\"\n",
    "    Returns:\n",
    "        bool: True if the repository exists else False\n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\"Authorization\": \"token \" + API_TOKEN}\n",
    "    url = f\"{GITHUB_API_URL}/repos/{repo}\"\n",
    "    r = requests.get(url, headers=headers)\n",
    "    return r.status_code == 200\n",
    "\n",
    "def get_forks(repo: str, count: int, page: int = 1) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of forks as \"user/repository_name\" from given base repository.\n",
    "    \n",
    "    Args:\n",
    "        repo (str): Name of the base repository given as \"user/repository_name\"\n",
    "        count (int): Number of forks to retrieve. Gets clamped to: min 1, max 100\n",
    "        page (int): Page from which to retrieve the forks. Defaults to the first page.\n",
    "    Returns:\n",
    "        list[str]: List of repository names given as \"user/repository_name\"\n",
    "    \"\"\"\n",
    "\n",
    "    if count <= 0:\n",
    "        count = 1\n",
    "    per_page = min(count, 100)\n",
    "    url = f\"{GITHUB_API_URL}/repos/{repo}/forks\"\n",
    "    headers = {\"Authorization\": \"token \" + API_TOKEN}\n",
    "    fork_urls = []\n",
    "\n",
    "    # Fetch forks until count is filled or no more is found\n",
    "    while len(fork_urls) < count:\n",
    "        params = {\"per_page\": per_page, \"page\": page}\n",
    "        r = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "        if r.status_code == 200:\n",
    "            forks = r.json()\n",
    "\n",
    "            # If no more forks are found, stop\n",
    "            if not forks:\n",
    "                break\n",
    "\n",
    "            for fork in forks:\n",
    "                # If count is reached, stop\n",
    "                if len(fork_urls) >= count:\n",
    "                    break\n",
    "                # full_name = e.g. vanna-ai/vanna\n",
    "                full_name = fork[\"full_name\"]\n",
    "                # Some forks may be unavailable so check them first\n",
    "                if repo_exists(full_name):\n",
    "                    print(\"Valid: \", full_name)\n",
    "                    fork_urls.append(full_name)\n",
    "                else:\n",
    "                    print(\"Unavailable: \", full_name)\n",
    "        else:\n",
    "            print(f\"Failed getting forks: {r.status_code}\")\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "\n",
    "    print(\"Forks found: \", str(len(fork_urls)))\n",
    "    print(forks)\n",
    "    return fork_urls\n",
    "\n",
    "def get_repos(count: int) -> None:\n",
    "    # Clone the base repository\n",
    "    clone_repo(BASE_REPO)\n",
    "\n",
    "    # Get base repo forks\n",
    "    forks = get_forks(BASE_REPO, count)\n",
    "\n",
    "    # Clone forks sources\n",
    "    clone_repos(forks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_repos(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM analysis\n",
    "\n",
    "Performs comparisons using LLM by taking two repositories, reading their source code files, fetching relevant snippets and prompting the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize functions and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "BASE_REPO = os.getenv(\"BASE_REPO\")\n",
    "API_TOKEN = os.getenv(\"GITHUB_API_TOKEN\")\n",
    "\n",
    "RESULT_DIR = \"results\"\n",
    "OUTPUT_DIR = \"repos\"\n",
    "\n",
    "if not os.path.exists(RESULT_DIR):\n",
    "    os.makedirs(RESULT_DIR)\n",
    "\n",
    "model_name = \"qwen2.5\"\n",
    "\n",
    "# Load the LLM model\n",
    "model = OllamaLLM(model=model_name, temperature=0.1)\n",
    "\n",
    "# Load embedding model\n",
    "embeddings = OllamaEmbeddings(model=model_name)\n",
    "\n",
    "def load_code_files(dir: str) -> list[Document]:\n",
    "    \"\"\"\n",
    "    Searches a given directory and its subdirectories for source code files.\n",
    "    Reads found files and wraps them in Document objects.\n",
    "\n",
    "    Includes files with file extensions shown in the file_extensions variable.\n",
    "    Ignores subdirectories in ignore_dirs variable.\n",
    "    \n",
    "    Args:\n",
    "        dir (str): Path of the directory to collect code files from\n",
    "    Returns:\n",
    "        list[Document]: List of document objects containing the source code files.\n",
    "    \"\"\"\n",
    "\n",
    "    file_extensions = ['.abap', '.asc', '.ash', '.ampl', '.mod', '.g4', '.apib', '.apl', '.dyalog', '.asp', '.asax', '.ascx', '.ashx', '.asmx', '.aspx', '.axd', '.dats', '.hats', '.sats', '.as', '.adb', '.ada', '.ads', '.agda', '.als', '.apacheconf', '.vhost', '.cls', '.applescript', '.scpt', '.arc', '.ino', '.asciidoc', '.adoc', '.asc', '.aj', '.asm', '.a51', '.inc', '.nasm', '.aug', '.ahk', '.ahkl', '.au3', '.awk', '.auk', '.gawk', '.mawk', '.nawk', '.bat', '.cmd', '.befunge', '.bison', '.bb', '.bb', '.decls', '.bmx', '.bsv', '.boo', '.b', '.bf', '.brs', '.bro', '.c', '.cats', '.h', '.idc', '.w', '.cs', '.cake', '.cshtml', '.csx', '.cpp', '.c++', '.cc', '.cp', '.cxx', '.h', '.h++', '.hh', '.hpp', '.hxx', '.inc', '.inl', '.ipp', '.tcc', '.tpp', '.c-objdump', '.chs', '.clp', '.cmake', '.cmake.in', '.cob', '.cbl', '.ccp', '.cobol', '.cpy', '.css', '.capnp', '.mss', '.ceylon', '.chpl', '.ch', '.ck', '.cirru', '.clw', '.icl', '.dcl', '.click', '.clj', '.boot', '.cl2', '.cljc', '.cljs', '.cljs.hl', '.cljscm', '.cljx', '.hic', '.coffee', '._coffee', '.cake', '.cjsx', '.cson', '.iced', '.cfm', '.cfml', '.cfc', '.lisp', '.asd', '.cl', '.l', '.lsp', '.ny', '.podsl', '.sexp', '.cp', '.cps', '.cl', '.coq', '.v', '.cppobjdump', '.c++-objdump', '.c++objdump', '.cpp-objdump', '.cxx-objdump', '.creole', '.cr', '.feature', '.cu', '.cuh', '.cy', '.pyx', '.pxd', '.pxi', '.d', '.di', '.d-objdump', '.com', '.dm', '.zone', '.arpa', '.d', '.darcspatch', '.dpatch', '.dart', '.diff', '.patch', '.dockerfile', '.djs', '.dylan', '.dyl', '.intr', '.lid', '.E', '.ecl', '.eclxml', '.ecl', '.sch', '.brd', '.epj', '.e', '.ex', '.exs', '.elm', '.el', '.emacs', '.emacs.desktop', '.em', '.emberscript', '.erl', '.es', '.escript', '.hrl', '.xrl', '.yrl', '.fs', '.fsi', '.fsx', '.fx', '.flux', '.f90', '.f', '.f03', '.f08', '.f77', '.f95', '.for', '.fpp', '.factor', '.fy', '.fancypack', '.fan', '.fs', '.for', '.eam.fs', '.fth', '.4th', '.f', '.for', '.forth', '.fr', '.frt', '.fs', '.ftl', '.fr', '.g', '.gco', '.gcode', '.gms', '.g', '.gap', '.gd', '.gi', '.tst', '.s', '.ms', '.gd', '.glsl', '.fp', '.frag', '.frg', '.fs', '.fsh', '.fshader', '.geo', '.geom', '.glslv', '.gshader', '.shader', '.vert', '.vrx', '.vsh', '.vshader', '.gml', '.kid', '.ebuild', '.eclass', '.po', '.pot', '.glf', '.gp', '.gnu', '.gnuplot', '.plot', '.plt', '.go', '.golo', '.gs', '.gst', '.gsx', '.vark', '.grace', '.gradle', '.gf', '.gml', '.graphql', '.dot', '.gv', '.man', '.1', '.1in', '.1m', '.1x', '.2', '.3', '.3in', '.3m', '.3qt', '.3x', '.4', '.5', '.6', '.7', '.8', '.9', '.l', '.me', '.ms', '.n', '.rno', '.roff', '.groovy', '.grt', '.gtpl', '.gvy', '.gsp', '.hcl', '.tf', '.hlsl', '.fx', '.fxh', '.hlsli', '.html', '.htm', '.html.hl', '.inc', '.st', '.xht', '.xhtml', '.mustache', '.jinja', '.eex', '.erb', '.erb.deface', '.phtml', '.http', '.hh', '.php', '.haml', '.haml.deface', '.handlebars', '.hbs', '.hb', '.hs', '.hsc', '.hx', '.hxsl', '.hy', '.bf', '.pro', '.dlm', '.ipf',  '.prefs', '.pro', '.properties', '.irclog', '.weechatlog', '.idr', '.lidr', '.ni', '.i7x', '.iss', '.io', '.ik', '.thy', '.ijs', '.flex', '.jflex', '.lock', '.topojson', '.jq', '.jsx', '.jade', '.j', '.java', '.jsp', '.js', '._js', '.bones', '.es', '.es6', '.frag', '.gs', '.jake', '.jsb', '.jscad', '.jsfl', '.jsm', '.jss', '.njs', '.pac', '.sjs', '.ssjs', '.sublime-build', '.sublime-commands', '.sublime-completions', '.sublime-keymap', '.sublime-macro', '.sublime-menu', '.sublime-mousemap', '.sublime-project', '.sublime-settings', '.sublime-theme', '.sublime-workspace', '.sublime_metrics', '.sublime_session', '.xsjs', '.xsjslib', '.jl', '.ipynb', '.krl', '.sch', '.brd', '.kicad_pcb', '.kit', '.kt', '.ktm', '.kts', '.lfe', '.ll', '.lol', '.lsl', '.lslp', '.lvproj', '.lasso', '.las', '.lasso8', '.lasso9', '.ldml', '.latte', '.lean', '.hlean', '.less', '.l', '.lex', '.ly', '.ily', '.b', '.m', '.ld', '.lds', '.mod', '.liquid', '.lagda', '.litcoffee', '.lhs', '.ls', '._ls', '.xm', '.x', '.xi', '.lgt', '.logtalk', '.lookml', '.ls', '.lua', '.fcgi', '.nse', '.pd_lua', '.rbxs', '.wlua', '.mumps', '.m', '.m4', '.m4', '.ms', '.mcr', '.mtml', '.muf', '.m', '.mak', '.d', '.mk', '.mkfile', '.mako', '.mao', '.ron', '.mask', '.mathematica', '.cdf', '.m', '.ma', '.mt', '.nb', '.nbp', '.wl', '.wlt', '.matlab', '.m', '.maxpat', '.maxhelp', '.maxproj', '.mxt', '.pat', '.mediawiki', '.wiki', '.m', '.moo', '.metal', '.minid', '.druby', '.duby', '.mir', '.mirah', '.mo', '.mod', '.mms', '.mmk', '.monkey', '.moo', '.moon', '.myt', '.ncl', '.nl', '.nsi', '.nsh', '.n', '.axs', '.axi', '.axs.erb', '.axi.erb', '.nlogo', '.nl', '.lisp', '.lsp', '.nginxconf', '.vhost', '.nim', '.nimrod', '.ninja', '.nit', '.nix', '.nu', '.numpy', '.numpyw', '.numsc', '.ml', '.eliom', '.eliomi', '.ml4', '.mli', '.mll', '.mly', '.objdump', '.m', '.h', '.mm', '.j', '.sj', '.omgrofl', '.opa', '.opal', '.cl', '.opencl', '.p', '.cls', '.scad', '.org', '.ox', '.oxh', '.oxo', '.oxygene', '.oz', '.pwn', '.inc', '.php', '.aw', '.ctp', '.fcgi', '.inc', '.php3', '.php4', '.php5', '.phps', '.phpt', '.pls', '.pck', '.pkb', '.pks', '.plb', '.plsql', '.sql', '.sql', '.pov', '.inc', '.pan', '.psc', '.parrot', '.pasm', '.pir', '.pas', '.dfm', '.dpr', '.inc', '.lpr', '.pp', '.pl', '.al', '.cgi', '.fcgi', '.perl', '.ph', '.plx', '.pm', '.pod', '.psgi', '.t', '.6pl', '.6pm', '.nqp', '.p6', '.p6l', '.p6m', '.pl', '.pl6', '.pm', '.pm6', '.t', '.pkl', '.l', '.pig', '.pike', '.pmod', '.pod', '.pogo', '.pony', '.ps', '.eps', '.ps1', '.psd1', '.psm1', '.pde', '.pl', '.pro', '.prolog', '.yap', '.spin', '.proto', '.asc', '.pub', '.pp', '.pd', '.pb', '.pbi', '.purs', '.py', '.bzl', '.cgi', '.fcgi', '.gyp', '.lmi', '.pyde', '.pyp', '.pyt', '.pyw', '.rpy', '.tac', '.wsgi', '.xpy', '.pytb', '.qml', '.qbs', '.pro', '.pri', '.r', '.rd', '.rsx', '.raml', '.rdoc', '.rbbas', '.rbfrm', '.rbmnu', '.rbres', '.rbtbar', '.rbuistate', '.rhtml', '.rmd', '.rkt', '.rktd', '.rktl', '.scrbl', '.rl', '.raw', '.reb', '.r', '.r2', '.r3', '.rebol', '.red', '.reds', '.cw', '.rpy', '.rs', '.rsh', '.robot', '.rg', '.rb', '.builder', '.fcgi', '.gemspec', '.god', '.irbrc', '.jbuilder', '.mspec', '.pluginspec', '.podspec', '.rabl', '.rake', '.rbuild', '.rbw', '.rbx', '.ru', '.ruby', '.thor', '.watchr', '.rs', '.rs.in', '.sas', '.scss', '.smt2', '.smt', '.sparql', '.rq', '.sqf', '.hqf', '.sql', '.cql', '.ddl', '.inc', '.prc', '.tab', '.udf', '.viw', '.sql', '.db2', '.ston', '.svg', '.sage', '.sagews', '.sls', '.sass', '.scala', '.sbt', '.sc', '.scaml', '.scm', '.sld', '.sls', '.sps', '.ss', '.sci', '.sce', '.tst', '.self', '.sh', '.bash', '.bats', '.cgi', '.command', '.fcgi', '.ksh', '.sh.in', '.tmux', '.tool', '.zsh', '.sh-session', '.shen', '.sl', '.slim', '.smali', '.st', '.cs', '.tpl', '.sp', '.inc', '.sma', '.nut', '.stan', '.ML', '.fun', '.sig', '.sml', '.do', '.ado', '.doh', '.ihlp', '.mata', '.matah', '.sthlp', '.styl', '.sc', '.scd', '.swift', '.sv', '.svh', '.vh', '.txl', '.tcl', '.adp', '.tm', '.tcsh', '.csh', '.tex', '.aux', '.bbx', '.bib', '.cbx', '.cls', '.dtx', '.ins', '.lbx', '.ltx', '.mkii', '.mkiv', '.mkvi', '.sty', '.toc', '.tea', '.t', '.fr', '.nb', '.ncl', '.no', '.textile', '.thrift', '.t', '.tu', '.ttl', '.twig', '.ts', '.tsx', '.upc', '.anim', '.asset', '.mat', '.meta', '.prefab', '.unity', '.uno', '.uc', '.ur', '.urs', '.vcl', '.vhdl', '.vhd', '.vhf', '.vhi', '.vho', '.vhs', '.vht', '.vhw', '.vala', '.vapi', '.v', '.veo', '.vim', '.vb', '.bas', '.cls', '.frm', '.frx', '.vba', '.vbhtml', '.vbs', '.volt', '.vue', '.owl', '.webidl', '.x10', '.xc', '.ant', '.axml', '.ccxml', '.clixml', '.cproject', '.csl', '.csproj', '.ct', '.dita', '.ditamap', '.ditaval', '.dll.config', '.dotsettings', '.filters', '.fsproj', '.fxml', '.glade', '.gml', '.grxml', '.iml', '.ivy', '.jelly', '.jsproj', '.kml', '.launch', '.mdpolicy', '.mm', '.mod', '.mxml', '.nproj', '.nuspec', '.odd', '.osm', '.plist', '.pluginspec', '.props', '.ps1xml', '.psc1', '.pt', '.rdf', '.rss', '.scxml', '.srdf', '.storyboard', '.stTheme', '.sublime-snippet', '.targets', '.tmCommand', '.tml', '.tmLanguage', '.tmPreferences', '.tmSnippet', '.tmTheme', '.ts', '.tsx', '.ui', '.urdf', '.ux', '.vbproj', '.vcxproj', '.vssettings', '.vxml', '.wsdl', '.wsf', '.wxi', '.wxl', '.wxs', '.x3d', '.xacro', '.xaml', '.xib', '.xlf', '.xliff', '.xmi', '.xml.dist', '.xproj', '.xsd', '.xul', '.zcml', '.xsp-config', '.xsp.metadata', '.xpl', '.xproc', '.xquery', '.xq', '.xql', '.xqm', '.xqy', '.xs', '.xslt', '.xsl', '.xojo_code', '.xojo_menu', '.xojo_report', '.xojo_script', '.xojo_toolbar', '.xojo_window', '.xtend', '.reek', '.rviz', '.sublime-syntax', '.syntax', '.yang', '.y', '.yacc', '.yy', '.zep', '.zimpl', '.zmpl', '.zpl', '.desktop', '.desktop.in', '.ec', '.eh', '.edn', '.fish', '.mu', '.nc', '.ooc', '.rst', '.rest', '.rest.txt', '.rst.txt', '.wisp', '.prg', '.ch', '.prw']\n",
    "    ignore_dirs = [\"tests\", \"docs\"]\n",
    "    source_code_files = []\n",
    "\n",
    "    # List files\n",
    "    for root, _, files in os.walk(dir):\n",
    "        # Skip irrelevant directories\n",
    "        if any(ignored in root for ignored in ignore_dirs):\n",
    "            continue\n",
    "\n",
    "        for f in files:\n",
    "            path = os.path.join(root, f)\n",
    "            # Read only source code files\n",
    "            for fe in file_extensions:\n",
    "                if f.endswith(fe):\n",
    "                    # print(path)\n",
    "                    with open(path, 'r', encoding='utf-8') as codefile:\n",
    "                        # Wrap source code into Document objects\n",
    "                        source_code_files.append(Document(page_content=codefile.read()))\n",
    "\n",
    "    return source_code_files\n",
    "\n",
    "def initialize_retriever(embeddings: OllamaEmbeddings, source_dir: str):\n",
    "    \"\"\"\n",
    "    Initializes a retriever for searching code documents using FAISS vector storage\n",
    "\n",
    "    Args:\n",
    "        embeddings: An embedding model used to convert text into vector representations.\n",
    "        source_dir (str): The directory containing the source code files to be indexed.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS-based retriever that enables semantic search over the source code documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get source code\n",
    "    source_code = load_code_files(source_dir)\n",
    "\n",
    "    # Create FAISS vectorstore\n",
    "    vectorstore = FAISS.from_documents(source_code, embeddings)\n",
    "\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "def compare_code(query: str, base_retriever, compare_retriever) -> str:\n",
    "    \"\"\"\n",
    "    Generates a comparison using code snippets from a base repository and a forked repository. \n",
    "\n",
    "    Args:\n",
    "        query: Query to be used for searching relevant code snippets\n",
    "        base_retriever: Retriever for the base repository\n",
    "        compare_retriever: Retriever for the forked repository\n",
    "    Returns:\n",
    "        str: Generated analysis as a string\n",
    "    \"\"\"\n",
    "\n",
    "    base_results = base_retriever.get_relevant_documents(query)\n",
    "    compare_results = compare_retriever.get_relevant_documents(query)\n",
    "\n",
    "    base_snippets = \"\\n\\n\".join([doc.page_content for doc in base_results])\n",
    "    compare_snippets = \"\\n\\n\".join([doc.page_content for doc in compare_results])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    ### **Instructions:**\n",
    "    You are given source code snippets from a base repository and a fork of that repository. \n",
    "\n",
    "    **Base Version:**\n",
    "    {base_snippets}\n",
    "\n",
    "    **Forked Version:**\n",
    "    {compare_snippets}\n",
    "\n",
    "    \n",
    "    Assess the degree of similarity between the two repositories:\n",
    "    - **Perform semantic comparisons between code segments**\n",
    "    - **Focus on identifying refactoring patterns and significant alterations**\n",
    "    - **Give a percentage of similarity between the repositories**\n",
    "\n",
    "    ### **Additional Instructions:**\n",
    "    - You must never hallucinate\n",
    "    - You have to always answer in English\n",
    "    - Make your response clear and structured\n",
    "    \"\"\"\n",
    "\n",
    "    analysis = model.invoke(prompt)\n",
    "    return analysis\n",
    "\n",
    "def generate_comparisions(base_retriever, base_dir: str, clear_json: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Generates comparisons between the given base directory and other repositories in the OUTPUT_DIR.\n",
    "    Saves generated analysis into a json object:\n",
    "    {\n",
    "        \"BASE\": \"user_repository_name\",\n",
    "        \"FORK\": \"user_repository_name\",\n",
    "        \"generation_time\": time it took to generate the analysis in seconds,\n",
    "        \"analysis\": analysis_result,\n",
    "    }\n",
    "\n",
    "    Generated json is saved into RESULTS_DIR/results.json\n",
    "\n",
    "    Args:\n",
    "        base_retriever: Retriever for the base repository\n",
    "        base_dir (str): Path to the base directory\n",
    "        clear_json (bool): If true, clears the existing results.json file. If false, appends generated content to the existing file. Default to False.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    query = \"Analyze the structure and patterns of this code.\"\n",
    "    results_path = f\"{RESULT_DIR}/results.json\"\n",
    "\n",
    "    # Load existing results data if the file exists\n",
    "    if clear_json:\n",
    "        data = []\n",
    "    else:\n",
    "        if os.path.exists(results_path):\n",
    "            with open(results_path, \"r\") as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                except json.JSONDecodeError:\n",
    "                    data = []\n",
    "        else:\n",
    "            data = []\n",
    "\n",
    "    # List files\n",
    "    for f in os.listdir(OUTPUT_DIR):\n",
    "        skip = False\n",
    "        # Skip comparing the base directory with itself\n",
    "        if f == base_dir:\n",
    "            continue\n",
    "\n",
    "        # Skip if analysis has been previously generated\n",
    "        for obj in data:\n",
    "            if obj.get(\"FORK\") == f:\n",
    "                print(\"Already generated for: \", f)\n",
    "                skip = True\n",
    "        if skip: continue\n",
    "\n",
    "        print(f\"Initializing retriver for: {OUTPUT_DIR}/{f}...\")\n",
    "        start_time = time.perf_counter()\n",
    "        COMPARE_RETRIEVER = initialize_retriever(embeddings, f\"{OUTPUT_DIR}/{f}\")\n",
    "\n",
    "        print(f\"Generating comparisons for: {OUTPUT_DIR}/{f}...\")\n",
    "        analysis_result = compare_code(query, base_retriever, COMPARE_RETRIEVER)\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed = end_time - start_time\n",
    "\n",
    "        # Generation_time contains setting up retrievers and generating analysis\n",
    "        data.append({\n",
    "            \"BASE\": base_dir,\n",
    "            \"FORK\": f,\n",
    "            \"generation_time\": elapsed,\n",
    "            \"analysis\": analysis_result,\n",
    "        })\n",
    "\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "def run_llm_analysis():\n",
    "    BASE_DIR = BASE_REPO.replace(\"/\", \"_\")\n",
    "    BASE_RETRIEVER = initialize_retriever(embeddings, f\"{OUTPUT_DIR}/{BASE_DIR}\")\n",
    "\n",
    "    generate_comparisions(BASE_RETRIEVER, BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_llm_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other useful functions\n",
    "\n",
    "Miscellaneous functions that can be used for quickly doing some common tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def json_to_csv(json_path: str, output_path: str = \"results/results_csv.csv\") -> None:\n",
    "    \"\"\"\n",
    "    Transforms the json data into a csv file\n",
    "\n",
    "    Args:\n",
    "        json_path: Path to the json results file\n",
    "        output_path: (Optional) output path for the csv\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    headers = data[0].keys()\n",
    "    with open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "json_to_csv(\"results/results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def calculate_total_generation_time(json_path: str) -> float:\n",
    "    \"\"\"\n",
    "    Sums the generation times from the results file\n",
    "\n",
    "    Args:\n",
    "        json_path: Path to the results file\n",
    "    Returns:\n",
    "        float: Total generation time in seconds\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return sum([result.get(\"generation_time\") for result in data])\n",
    "\n",
    "print(calculate_total_generation_time(\"results/results.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_analysis_for_fork(json_path: str, fork_name: str):\n",
    "    \"\"\"\n",
    "    Gets the generated analysis for the given fork name\n",
    "\n",
    "    Args:\n",
    "        json_path: Path to the results file\n",
    "        fork_name: Name of the fork given as \"user_repository\"\n",
    "    Returns:\n",
    "        Generated analysis OR\n",
    "        None if the name is not found in the results\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for obj in data:\n",
    "        if obj.get(\"FORK\") == fork_name:\n",
    "            return obj.get(\"analysis\")\n",
    "\n",
    "    return None\n",
    "\n",
    "example = get_analysis_for_fork(\"results/results.json\", \"ZebinLiu_vanna\")\n",
    "if example: print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# ---TESTS---\n",
    "TEST_JSON = \"test_data/test_results.json\"\n",
    "\n",
    "def test_repo_exists():\n",
    "    valid_result = repo_exists(\"vanna-ai/vanna\")\n",
    "    invalid_result = repo_exists(\"aaaaaaaa\")\n",
    "\n",
    "    assert valid_result, \"Checking valid repository failed\"\n",
    "    assert not invalid_result, \"Checking invalid repository failed\"\n",
    "\n",
    "    print(\"Test passed: test_repo_exists\")\n",
    "\n",
    "def test_clone_repo():\n",
    "    valid_repo = \"Ferkku/Advanced-Software-Quality\"\n",
    "    invalid_repo = \"asdf\"\n",
    "    valid_name = valid_repo.replace(\"/\", \"_\")\n",
    "    invalid_name = invalid_repo.replace(\"/\", \"_\")\n",
    "    \n",
    "    clone_repo(valid_repo, \"test_output\")\n",
    "    clone_repo(invalid_repo, \"test_output\")\n",
    "\n",
    "    assert os.path.exists(f\"test_output/{valid_name}\"), \"Cloning valid repository failed\"\n",
    "    assert not os.path.exists(f\"test_output/{invalid_name}\"), \"Cloning invalid repository didn't cause an error\"\n",
    "\n",
    "    print(\"Test passed: test_clone_repo\")\n",
    "\n",
    "def test_get_forks():\n",
    "    count = 10\n",
    "    forks = get_forks(\"vanna-ai/vanna\", count)\n",
    "\n",
    "    assert len(forks) == count, f\"Forks fetched was {len(forks)}, should've been {count}\"\n",
    "\n",
    "    print(\"Test passed: test_get_forks\")\n",
    "\n",
    "\n",
    "def test_json_to_csv():\n",
    "    output_path = \"test_output/test_results_csv.csv\"\n",
    "\n",
    "    expected_data = [\n",
    "        [\"BASE\", \"FORK\", \"generation_time\", \"analysis\"],\n",
    "        [\"test_base\", \"test_fork1\", \"10.5\", \"test json output 1\"],\n",
    "        [\"test_base\", \"test_fork2\", \"20.0\", \"test json output 2\"],\n",
    "        [\"test_base\", \"test_fork3\", \"5.8\", \"test json output 3\"],\n",
    "    ]\n",
    "\n",
    "    json_to_csv(TEST_JSON, output_path)\n",
    "\n",
    "    assert os.path.exists(output_path), \"Creating csv failed: path not found\"\n",
    "\n",
    "    with open(output_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        read_data = [row for row in reader]\n",
    "\n",
    "    assert expected_data == read_data, \"Created csv has different data than expected\"\n",
    "\n",
    "    # Clean up\n",
    "    os.remove(output_path)\n",
    "\n",
    "    print(\"Test passed: test_json_to_csv\")\n",
    "\n",
    "def test_calculate_total_generation_time():\n",
    "    result = calculate_total_generation_time(TEST_JSON)\n",
    "\n",
    "    assert result == 36.3\n",
    "\n",
    "    print(\"Test passed: test_calculate_total_generation_time\")\n",
    "\n",
    "def test_get_analysis_for_fork():\n",
    "    result = get_analysis_for_fork(TEST_JSON, \"test_fork2\")\n",
    "    expected = \"test json output 2\"\n",
    "\n",
    "    assert result == expected, f\"Fetched result: {result}, differs from expected: {expected}\"\n",
    "\n",
    "    print(\"Test passed: test_get_analysis_for_fork\")\n",
    "\n",
    "def run_tests():\n",
    "    test_repo_exists()\n",
    "    test_clone_repo()\n",
    "    test_get_forks()\n",
    "    test_json_to_csv()\n",
    "    test_calculate_total_generation_time()\n",
    "    test_get_analysis_for_fork()\n",
    "\n",
    "run_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
