{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-ollama\n",
    "!pip install faiss-cpu\n",
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests==2.32.3 in c:\\users\\jyrki\\documents\\repos\\advanced-software-quality\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (2.32.3)\n",
      "Collecting python-dotenv==1.0.1 (from -r requirements.txt (line 2))\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jyrki\\documents\\repos\\advanced-software-quality\\.venv\\lib\\site-packages (from requests==2.32.3->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jyrki\\documents\\repos\\advanced-software-quality\\.venv\\lib\\site-packages (from requests==2.32.3->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jyrki\\documents\\repos\\advanced-software-quality\\.venv\\lib\\site-packages (from requests==2.32.3->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jyrki\\documents\\repos\\advanced-software-quality\\.venv\\lib\\site-packages (from requests==2.32.3->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning: https://github.com/kgabis/parson\n",
      "Cloning: https://github.com/hibiscus-desgin/parson\n",
      "Cloning: https://github.com/AKJUS/parson\n",
      "Cloning: https://github.com/sayo9394/parson\n",
      "Cloning: https://github.com/soft9000/JSONRead4C\n",
      "Cloning: https://github.com/gmh5225/parson\n",
      "Cloning: https://github.com/zhuqi77hub/parson\n",
      "Cloning: https://github.com/lizhuo-eric/parson\n",
      "Cloning: https://github.com/oss-evaluation-repository/kgabis-parson\n",
      "Cloning: https://github.com/aiworkspace/parson\n",
      "Cloning: https://github.com/DavidKorczynski/parson\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import subprocess\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "GITHUB_API_URL=\"https://api.github.com\"\n",
    "OUTPUT_DIR = \"repos\"\n",
    "\n",
    "load_dotenv()\n",
    "BASE_REPO = os.getenv(\"BASE_REPO\")\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "\n",
    "# Clones a single repo from github to OUTPUT_DIR\n",
    "def clone_repo(repo):\n",
    "    dir_name = repo.replace(\"/\", \"_\")\n",
    "    clone_path = f\"{OUTPUT_DIR}/{dir_name}\"\n",
    "    # If folder does not exist, clone the repository\n",
    "    if not os.path.exists(clone_path):\n",
    "        print(f\"Cloning: https://github.com/{repo}\")\n",
    "        subprocess.run([\"git\", \"clone\", f\"https://github.com/{repo}\", dir_name], cwd=OUTPUT_DIR, check=True)\n",
    "    else:\n",
    "        print(f\"Already cloned: {clone_path}\")\n",
    "\n",
    "# Clone all repositories from a list\n",
    "def clone_repos(repos):\n",
    "    for repo in repos:\n",
    "        clone_repo(repo)\n",
    "\n",
    "# Returns a list of fork urls from given repository\n",
    "def get_forks(repo, count):\n",
    "    page = 1\n",
    "    per_page = min(count, 100)\n",
    "    params = {\"per_page\": per_page}\n",
    "    url = f\"{GITHUB_API_URL}/repos/{repo}/forks?page={page}\"\n",
    "\n",
    "    r = requests.get(url, params=params)\n",
    "    if r.status_code == 200:\n",
    "        forks = r.json()\n",
    "        fork_urls = []\n",
    "        for fork in forks:\n",
    "            # full_name = e.g. kgabis/parson\n",
    "            fork_urls.append(fork[\"full_name\"])\n",
    "        return fork_urls\n",
    "    else:\n",
    "        print(f\"Failed getting forks: {r.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Clone the base repository\n",
    "clone_repo(BASE_REPO)\n",
    "\n",
    "# Get base repo forks\n",
    "forks = get_forks(BASE_REPO, 10)\n",
    "\n",
    "# Clone forks sources\n",
    "clone_repos(forks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "model_name = \"marco-o1\"\n",
    "\n",
    "def load_code_files(dir):\n",
    "    source_code_files = []\n",
    "\n",
    "    # List files\n",
    "    for f in os.listdir(dir):\n",
    "        # Read only C files (for now)\n",
    "        if f.endswith(\".c\"):\n",
    "            with open(os.path.join(dir, f), 'r', encoding='utf-8') as codefile:\n",
    "                # Wrap source code into Document objects\n",
    "                source_code_files.append(Document(page_content=codefile.read()))\n",
    "\n",
    "    return source_code_files\n",
    "\n",
    "BASE_SOURCE_CODE = load_code_files(f\"{OUTPUT_DIR}/kgabis_parson\")\n",
    "# Testing with random fork\n",
    "COMPARE_SOURCE_CODE = load_code_files(f\"{OUTPUT_DIR}/soft9000_JSONRead4C\")\n",
    "\n",
    "# Load embedding model\n",
    "embeddings = OllamaEmbeddings(model=model_name)\n",
    "\n",
    "# Create FAISS vector stores\n",
    "base_vectorstore = FAISS.from_documents(BASE_SOURCE_CODE, embeddings)\n",
    "compare_vectorstore = FAISS.from_documents(COMPARE_SOURCE_CODE, embeddings)\n",
    "\n",
    "# Save and reload the vector stores\n",
    "base_vectorstore.save_local(\"faiss_index_base\")\n",
    "base_persisted_vectorstore = FAISS.load_local(\"faiss_index_base\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "compare_vectorstore.save_local(\"faiss_index_compare\")\n",
    "compare_persisted_vectorstore = FAISS.load_local(\"faiss_index_compare\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Create a retrievers\n",
    "base_retriever = base_persisted_vectorstore.as_retriever()\n",
    "compare_retriever = compare_persisted_vectorstore.as_retriever()\n",
    "\n",
    "# Load the LLM model\n",
    "model = OllamaLLM(model=model_name, temperature=0.1)\n",
    "\n",
    "def compare_code(query):\n",
    "    base_results = base_retriever.get_relevant_documents(query)\n",
    "    compare_results = compare_retriever.get_relevant_documents(query)\n",
    "\n",
    "    base_snippets = \"\\n\\n\".join([doc.page_content for doc in base_results])\n",
    "    compare_snippets = \"\\n\\n\".join([doc.page_content for doc in compare_results])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    ### **Instructions:**\n",
    "    You are given source code from a base repository and a fork of that repository. \n",
    "\n",
    "    **Base Version:**\n",
    "    {base_snippets}\n",
    "\n",
    "    **Forked Version:**\n",
    "    {compare_snippets}\n",
    "\n",
    "    \n",
    "    Assess the degree of similarity between the two repositories:\n",
    "    - **Perform semantic comparisons between code segments**\n",
    "    - **Focus on identifying refactoring patterns and significant alterations**\n",
    "\n",
    "    ### **Additional Instructions:**\n",
    "    - You must never hallucinate\n",
    "    - You have to always answer in English\n",
    "    - Make your response clear and structured\n",
    "    \"\"\"\n",
    "\n",
    "    analysis = model.invoke(prompt)\n",
    "    return analysis\n",
    "\n",
    "query = \"Analyze the structure and patterns of this code.\"\n",
    "analysis_result = compare_code(query)\n",
    "\n",
    "print(\"LLM Analysis:\\n\", analysis_result)\n",
    "with open(\"results.txt\", 'w', encoding='utf-8') as f:\n",
    "    f.write(analysis_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
